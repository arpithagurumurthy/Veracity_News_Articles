{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1356b1f891ed46deb4cabd99e85ba0b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "state": {
            "_view_name": "ButtonView",
            "style": "IPY_MODEL_4204160f4bf241878acb8b0c57bb2b5d",
            "_dom_classes": [],
            "description": "Get Feed",
            "_model_name": "ButtonModel",
            "button_style": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "tooltip": "",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_a27e24b284c84533afe81fdd2f34ae1a",
            "_model_module": "@jupyter-widgets/controls",
            "icon": ""
          },
          "model_module_version": "1.5.0"
        },
        "4204160f4bf241878acb8b0c57bb2b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ButtonStyleModel",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "button_color": null,
            "font_weight": "",
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "a27e24b284c84533afe81fdd2f34ae1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "10dc9fef0b6a464280646650be1a118e": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "state": {
            "_view_name": "OutputView",
            "msg_id": "",
            "_dom_classes": [],
            "_model_name": "OutputModel",
            "outputs": [],
            "_view_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_view_count": null,
            "_view_module_version": "1.0.0",
            "layout": "IPY_MODEL_be9d7e1691484a09a561547b5fb8d115",
            "_model_module": "@jupyter-widgets/output"
          },
          "model_module_version": "1.0.0"
        },
        "be9d7e1691484a09a561547b5fb8d115": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur8QmMpY-UN3"
      },
      "source": [
        "## **Team Amalgam**\n",
        "Surabhi - Factor credibility\n",
        "\n",
        "Gayathri - Factor Authenticity\n",
        "\n",
        "Arpitha - Factor Style Based Approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNelSyXenphR"
      },
      "source": [
        "# Necessary imports\n",
        "Installing required libraries and scraping data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlUwcvwzjv1O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa2b3a9c-55d5-4dbe-dcc2-f0047b714a81"
      },
      "source": [
        "!pip install textstat\n",
        "!pip install nltk\n",
        "!pip install spacy-readability\n",
        "!pip install feedparser\n",
        "!pip3 install newspaper3k\n",
        "!pip install texthero\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textstat\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/b1/ab40a00b727a0d209402d1be6aa3f1bc75bd03678b59ace8507b08bf12f5/textstat-0.7.0-py3-none-any.whl (99kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 6.1MB/s \n",
            "\u001b[?25hCollecting pyphen\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/5a/5bc036e01389bc6a6667a932bac3e388de6e7fa5777a6ff50e652f60ec79/Pyphen-0.10.0-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 21.8MB/s \n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.10.0 textstat-0.7.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Collecting spacy-readability\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/9e/e8d9cdf0d54fa5fa0c6463bc6d0385c37deb5dc65a4cfe2c612a02a06869/spacy_readability-1.4.1-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.8MB/s \n",
            "\u001b[?25hCollecting syllapy<1,>=0\n",
            "  Downloading https://files.pythonhosted.org/packages/11/31/e13c6b0ed7a95f46c3af20df2b995877ea179b88a90ec39122e0621dae08/syllapy-0.7.1-py3-none-any.whl\n",
            "Requirement already satisfied: spacy<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from spacy-readability) (2.2.4)\n",
            "Collecting ujson<2.0,>=1.35\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 29.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->spacy-readability) (56.1.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->spacy-readability) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->spacy-readability) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->spacy-readability) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->spacy-readability) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->spacy-readability) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->spacy-readability) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->spacy-readability) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->spacy-readability) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->spacy-readability) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->spacy-readability) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->spacy-readability) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->spacy-readability) (3.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0,>=2.0->spacy-readability) (4.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0,>=2.0->spacy-readability) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0,>=2.0->spacy-readability) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0,>=2.0->spacy-readability) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0,>=2.0->spacy-readability) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0,>=2.0->spacy-readability) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0,>=2.0->spacy-readability) (3.7.4.3)\n",
            "Building wheels for collected packages: ujson\n",
            "  Building wheel for ujson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ujson: filename=ujson-1.35-cp37-cp37m-linux_x86_64.whl size=68394 sha256=e659da6faf26f4bcfe867b7b5b54f240c083137ed8290371003a4e96f7b4b763\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/77/e4/0311145b9c2e2f01470e744855131f9e34d6919687550f87d1\n",
            "Successfully built ujson\n",
            "Installing collected packages: ujson, syllapy, spacy-readability\n",
            "Successfully installed spacy-readability-1.4.1 syllapy-0.7.1 ujson-1.35\n",
            "Collecting feedparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/21/faf1bac028662cc8adb2b5ef7a6f3999a765baa2835331df365289b0ca56/feedparser-6.0.2-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.0MB/s \n",
            "\u001b[?25hCollecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp37-none-any.whl size=6067 sha256=31f8630b9e43bab91fb5307b5e5568038d33449eb5535f1cc06d181d3c6aea44\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.2 sgmllib3k-1.0.0\n",
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 13.5MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\n",
            "Collecting jieba3k>=0.35.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 27.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\n",
            "Collecting tldextract>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/62/b6acd3129c5615b9860e670df07fd55b76175b63e6b7f68282c7cad38e9e/tldextract-3.1.0-py2.py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (6.0.2)\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.1->newspaper3k) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2020.12.5)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.0.12)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Building wheels for collected packages: jieba3k, tinysegmenter, feedfinder2\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp37-none-any.whl size=7398406 sha256=70d493e98129dd647ad5ce3a8246712a7e1be7ec0864ec3ae34689ea995f45fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp37-none-any.whl size=13538 sha256=e659175509c81a6edfcfe61c855e15bc0cd39b93f627d43f19e57142749cf612\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp37-none-any.whl size=3358 sha256=e41302b81e2aa24b7d09b8c3b16ab72fb57f10f09f2d76857e6170ea8ad454a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "Successfully built jieba3k tinysegmenter feedfinder2\n",
            "Installing collected packages: cssselect, jieba3k, tinysegmenter, requests-file, tldextract, feedfinder2, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 tinysegmenter-0.3 tldextract-3.1.0\n",
            "Collecting texthero\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/5a/a9d33b799fe53011de79d140ad6d86c440a2da1ae8a7b24e851ee2f8bde8/texthero-1.0.9-py3-none-any.whl\n",
            "Requirement already satisfied: pandas>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from texthero) (0.22.2.post1)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.2.2)\n",
            "Collecting nltk>=3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 19.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wordcloud>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.5.0)\n",
            "Requirement already satisfied: plotly>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (4.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.7/dist-packages (from texthero) (4.41.1)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from texthero) (2.2.4)\n",
            "Collecting unidecode>=1.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 39.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.2->texthero) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.2->texthero) (2018.9)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->texthero) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->texthero) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (0.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (8.0.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (2019.12.20)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud>=1.5.0->texthero) (7.1.2)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.2.0->texthero) (1.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly>=4.2.0->texthero) (1.15.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (56.1.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (1.1.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.6.0->texthero) (5.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->texthero) (4.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->texthero) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->texthero) (3.7.4.3)\n",
            "Installing collected packages: nltk, unidecode, texthero\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.2 texthero-1.0.9 unidecode-1.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtfI6jIOXaTY",
        "outputId": "3c65ab76-a451-4e7c-86cb-2c915127110d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRZ0Gxg7qALJ",
        "outputId": "784f2a66-2c26-4f09-88c8-2926c1f078e4"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/MLSpring-2021/teams/amalgam/Pickle_TeamIntegration"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/gdrive/MyDrive/MLSpring-2021/teams/amalgam/Pickle_TeamIntegration'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97mjBhIC-y9n",
        "outputId": "c52166e0-ddca-4db7-fa76-df59061cc87d"
      },
      "source": [
        "!python -m textblob.download_corpora"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5hVK3acjpfP"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from tabulate import tabulate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sck4ZhdIxRA2",
        "outputId": "5e1cc83c-d9c2-4e38-d840-20b189827621"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWpkpYUiEG3e",
        "outputId": "c9168b59-f68a-4614-dd4d-c5cd9e2dc5cc"
      },
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI2mlsHpj5UG"
      },
      "source": [
        "#Import the dependencies\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests\n",
        "import urllib.request\n",
        "import time\n",
        "\n",
        "from io import BytesIO\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer #for stemming\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk.corpus\n",
        "\n",
        "import time\n",
        "\n",
        "import datetime\n",
        "import calendar\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import  LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn import svm\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "import pickle\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import datetime\n",
        "timestr = datetime.datetime.now().strftime(\"%Y-%m-%d_%I-%M-%S_%p\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeyW0_HfxJju"
      },
      "source": [
        "# **SURABHI** - Credibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujUoIwNpGIpE"
      },
      "source": [
        "## Readability + Context relevance + Site Score(Author Expertise calculated using Named Entity Recognition Score)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-pNBzB2xM-1"
      },
      "source": [
        "import spacy\n",
        "from spacy_readability import Readability\n",
        "import texthero as hero\n",
        "from texthero import preprocessing\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "read = Readability()\n",
        "nlp.add_pipe(read, last=True)\n",
        "\n",
        "\n",
        "# for natural language processing: named entity recognition\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "# for visualizations\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQIagPzgkBQK"
      },
      "source": [
        "a_dict = {}\n",
        "\n",
        "def calc_text_readability_score(sent):\n",
        "  doc = nlp(sent)\n",
        "  dale_chall_score = doc._.dale_chall\n",
        "  readability_index = doc._.automated_readability_index\n",
        "  return pd.Series([dale_chall_score,readability_index],index=['dale_chall_score','readability_index'])\n",
        "\n",
        "def similar_docs(sent, model):\n",
        "  label = []\n",
        "  score = []\n",
        "  sent = sent.split(' ')\n",
        "  for m in model.docvecs.most_similar([model.infer_vector(sent)],  topn = 5):\n",
        "    label.append(m[0])\n",
        "    score.append(round(m[1],2))\n",
        "    # print(label)\n",
        "    # print(score)\n",
        "  return pd.Series([label,score],index=['label','score'])\n",
        "\n",
        "def similar_docs_web(sent, model):\n",
        "  label = []\n",
        "  score = []\n",
        "  sent = sent.split(' ')\n",
        "  for m in model.docvecs.most_similar([model.infer_vector(sent)]):\n",
        "    label.append(m[0])\n",
        "    score.append(round(m[1],2))\n",
        "    # print(label)\n",
        "    # print(score)\n",
        "  return pd.Series([label,score],index=['label','score'])\n",
        "\n",
        "def calc_text_readability_score(sent):\n",
        "  doc = nlp(sent)\n",
        "  dale_chall_score = doc._.dale_chall\n",
        "  readability_index = doc._.automated_readability_index\n",
        "  return pd.Series([dale_chall_score,readability_index],index=['dale_chall_score','readability_index'])\n",
        "\n",
        "def get_ner_dict(df, text):\n",
        "  tokens = nlp(''.join(str(df.text.tolist())))\n",
        "\n",
        "  items = [x.text for x in tokens.ents]\n",
        "  a_dict = dict(Counter(items).most_common(200))\n",
        "\n",
        "def find_ner_count(title):\n",
        "  tokens = title.split(\" \")\n",
        "  count = 0\n",
        "  for token in tokens:\n",
        "    for key, val in a_dict.items():\n",
        "      if key == token:\n",
        "        count += val\n",
        "  return count\n",
        "\n",
        "#text cleaning for doc2vec model \n",
        "\n",
        "\n",
        "custom_pipeline = [preprocessing.fillna,\n",
        "                   preprocessing.lowercase,\n",
        "                   preprocessing.remove_whitespace,\n",
        "                   preprocessing.remove_diacritics,\n",
        "                   #preprocessing.remove_brackets\n",
        "                  ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-AvvA9wkGtN"
      },
      "source": [
        "## Latent features - ner_count, avg_cos_score, dale_chall_score, readability_index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaX0hObBkERR"
      },
      "source": [
        "def credibility_preprocessing(streaming_df):\n",
        "  streaming_df = streaming_df.dropna()\n",
        "  ##Cleaning text in summary feature\n",
        "  streaming_df['clean_text'] = hero.clean(streaming_df['Summary'], custom_pipeline)\n",
        "  streaming_df['clean_text'] = [n.replace('{','') for n in streaming_df['clean_text']]\n",
        "  streaming_df['clean_text'] = [n.replace('}','') for n in streaming_df['clean_text']]\n",
        "  streaming_df['clean_text'] = [n.replace('(','') for n in streaming_df['clean_text']]\n",
        "  streaming_df['clean_text'] = [n.replace(')','') for n in streaming_df['clean_text']]\n",
        "  streaming_df['clean_text'] = [n.replace('[','') for n in streaming_df['clean_text']]\n",
        "  streaming_df['clean_text'] = [n.replace(']','') for n in streaming_df['clean_text']]\n",
        "\n",
        "  ##Calculating distilled feature - ner count\n",
        "  streaming_df['ner_count'] = streaming_df.apply(lambda row : find_ner_count(row['Summary']), axis=1)\n",
        "\n",
        "  from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "  #tokenize comments\n",
        "  comment_docs = [TaggedDocument(doc.split(' '), [i]) \n",
        "              for i, doc in enumerate(streaming_df.clean_text)]\n",
        "\n",
        "  poli_model = Doc2Vec(dm = 1, min_count=1, window=6, size=150, sample=1e-4, negative=10)\n",
        "  poli_model.build_vocab(comment_docs)\n",
        "\n",
        "  ##Calculating feature\n",
        "  streaming_df[['tag','score']] = streaming_df.apply(lambda row : similar_docs(row['clean_text'], poli_model), axis=1)\n",
        "  streaming_df['avg_cos_score'] = streaming_df.apply(lambda row : np.mean(row['score']), axis=1)\n",
        "  streaming_df[['dale_chall_score','readability_index']] = streaming_df.apply(lambda row : calc_text_readability_score(row['clean_text']), axis=1)\n",
        "\n",
        "  streaming_df.to_csv('csv/credibility_preprocess_df.csv')\n",
        "\n",
        "\n",
        "def invoke_modeling_credibility():\n",
        "  credibility_preprocess_df = pd.read_csv('csv/credibility_preprocess_df.csv')\n",
        "\n",
        "  X_test = credibility_preprocess_df[['ner_count','avg_cos_score','dale_chall_score','readability_index']]\n",
        "  credibility_preds = []\n",
        "  credibility_model = pickle.load(open(\"models/credibility_model.pkl\", \"rb\" ) )\n",
        "  credibility_preds = credibility_model.predict(X_test)\n",
        "  credibility_preprocess_df['credibility_factor'] = credibility_preds\n",
        "\n",
        "  credibility_preprocess_df.to_csv('csv/credibility_df.csv')\n",
        "  streaming_df = pd.read_csv('csv/results_streaming.csv')\n",
        "  streaming_df = pd.merge(streaming_df, credibility_preprocess_df,  on = ['Posted','Link','Summary','Headline','Source'])\n",
        "  streaming_df = streaming_df[['Posted','Link','Summary','Headline','Source','AuthenticityScore','credibility_factor']]\n",
        "  streaming_df.to_csv('csv/results_streaming.csv')\n",
        "  print(streaming_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roEVzO8Jw71P"
      },
      "source": [
        "# **GAYATHRI** - Authenticity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2HpD4nx_J7-"
      },
      "source": [
        "## Flesch Reading Ease Score + polarity score + subjectivity score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9B0Tpjvw_87"
      },
      "source": [
        "def authenticity_preprocessing(streaming_df):\n",
        "  \n",
        "  import textstat\n",
        "  import pickle\n",
        "  from textblob import TextBlob\n",
        "\n",
        "  #calculating flesch score based on the comment's complexity using textstat\n",
        "  streaming_df['FleschScore'] = streaming_df['Summary'].map(lambda x: textstat.flesch_reading_ease((x)))\n",
        "\n",
        "  #getting polarity score which ranges from -1 to +1 \n",
        "  def get_polarity_score(text):\n",
        "    doc = TextBlob(text)\n",
        "    return doc.polarity\n",
        "\n",
        "  #getting subjectivity score which ranges from 0 to 1, states how subjective or objective the text is\n",
        "  def get_subjectivity_score(text):\n",
        "    doc = TextBlob(text)\n",
        "    return doc.subjectivity\n",
        "\n",
        "  streaming_df['PolarityScore'] = streaming_df['Summary'].apply(get_polarity_score)\n",
        "  streaming_df['SubjectivityScore'] = streaming_df['Summary'].apply(get_subjectivity_score)\n",
        "\n",
        "  print('authenticity preprocessing done!!')\n",
        "    \n",
        "  ##Convert preprocessed df to csv\n",
        "  streaming_df.to_csv('csv/fleschScore_preprocess_df.csv')\n",
        "\n",
        "\n",
        "def invoke_modeling_authenticity():\n",
        "  fleschScore_preprocess_df = pd.read_csv('csv/fleschScore_preprocess_df.csv')\n",
        "  flesch_features = ['FleschScore','PolarityScore', 'SubjectivityScore']\n",
        "  X_flesch = fleschScore_preprocess_df.loc[: ,flesch_features]\n",
        "\n",
        "  flesch_preds = []\n",
        "  flesch_model = pickle.load( open( \"models/authenticity_model.pkl\", \"rb\" ) )\n",
        "  flesch_preds = flesch_model.predict(X_flesch)\n",
        "  fleschScore_preprocess_df['AuthenticityScore'] = flesch_preds\n",
        "\n",
        "  fleschScore_preprocess_df.to_csv('csv/autheticity.csv')\n",
        "  print(\"autheticity score calculation done!\")\n",
        "\n",
        "  streaming_df = pd.read_csv('csv/results_streaming.csv')\n",
        "  streaming_df = pd.merge(streaming_df, fleschScore_preprocess_df,  on = ['Posted','Link','Summary','Headline','Source'])\n",
        "  streaming_df = streaming_df[['Posted','Link','Summary','Headline','Source','AuthenticityScore']]\n",
        "  streaming_df.to_csv('csv/results_streaming.csv')\n",
        "  # print(tabulate(streaming_df, headers='keys', tablefmt='psql'))\n",
        "  print(streaming_df)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXhfxpyswy4u"
      },
      "source": [
        "# **ARPITHA** - Style based approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azuPjyeigAao"
      },
      "source": [
        "## Yellow Factor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qj8YGZHf_ci"
      },
      "source": [
        "#function to find if string contains a question and if so, create new feature with a 1 for yes or 0 for no\n",
        "question_words = ['who','what','where','why','when','whose','whom','would','will','how','which','should','could']\n",
        "\n",
        "def is_question(text):\n",
        "    if \"?\" in text or text.startswith(('who','what','where','why','when','whose','whom','would','will','how','which','should','could','did','do')):\n",
        "        return 1\n",
        "    else: \n",
        "        return 0\n",
        "\n",
        "#create function to find if headline contains '!' and create new feature with 1 for yes and 0 for no\n",
        "def is_exclamation(headline):\n",
        "    if \"!\" in headline: \n",
        "        return 1\n",
        "    else: \n",
        "        return 0\n",
        "\n",
        "#create function to find if headline contains '$' and create new feature with 1 for yes and 0 for no\n",
        "def is_money_related(headline):\n",
        "    if \"$\" in headline: \n",
        "        return 1\n",
        "    else: \n",
        "        return 0\n",
        "\n",
        "#create function to find if headline starts with a digit and create new feature with 1 for yes and 0 for no\n",
        "def starts_with_num(text):\n",
        "    if text.startswith(('1','2','3','4','5','6','7','8','9')): \n",
        "        return 1\n",
        "    else: \n",
        "        return 0\n",
        "\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
        "\n",
        "def yellowness_preprocessing(yellow_preprocess_df):\n",
        "  \n",
        "    print(\"Starting preprocessing for yellowness factor on our text..\")\n",
        "    yellow_preprocess_df.Summary = yellow_preprocess_df.Summary.apply(str)\n",
        "\n",
        "    ##Creating a feature to score the text with a readability factor\n",
        "    yellow_preprocess_df['flesch'] = yellow_preprocess_df['Summary'].apply(textstat.flesch_reading_ease)\n",
        "    ##Since the flesch reading score is now inversely proportional to the yellowness factor\n",
        "    ##we substract its value from the highest - 121.22\n",
        "    yellow_preprocess_df['flesch'] = 121.22 - yellow_preprocess_df['flesch'] \n",
        "\n",
        "    #function to find if string contains a question and if so, create new feature with a 1 for yes or 0 for no\n",
        "    yellow_preprocess_df['text_lower']=yellow_preprocess_df['Summary'].apply(lambda x: x.lower())\n",
        "    yellow_preprocess_df['is_q']=yellow_preprocess_df['text_lower'].apply(is_question)\n",
        "    yellow_preprocess_df = yellow_preprocess_df.drop(columns='text_lower')\n",
        "\n",
        "    #create function to find if headline contains '!' and create new feature with 1 for yes and 0 for no\n",
        "    yellow_preprocess_df['is_exclam']=yellow_preprocess_df['Summary'].apply(is_exclamation)\n",
        "\n",
        "    #create function to find if headline contains '$' and create new feature with 1 for yes and 0 for no\n",
        "    yellow_preprocess_df['is_money']=yellow_preprocess_df['Summary'].apply(is_money_related)\n",
        "\n",
        "    #create function to find if headline starts with a digit and create new feature with 1 for yes and 0 for no\n",
        "    yellow_preprocess_df['starts_num']=yellow_preprocess_df['Summary'].apply(starts_with_num)\n",
        "\n",
        "    ##Calculating the number of words in each Summary \n",
        "    yellow_preprocess_df['num_words'] = yellow_preprocess_df['Summary'].apply(lambda x: len(x.split()))\n",
        "\n",
        "    ##Creating a new feature to store the number of ALL CAPS words in each Summary\n",
        "    yellow_preprocess_df['All_Caps'] = yellow_preprocess_df['Summary'].apply(lambda x: sum(map(str.isupper, x.split())))\n",
        "\n",
        "    ##Looping to pass every headline into the analyser, a sentiment score is assigned to each headline.\n",
        "    sia = SIA()\n",
        "    results = []\n",
        "    for headline in yellow_preprocess_df['Summary']:\n",
        "        pol_score = sia.polarity_scores(headline)\n",
        "        pol_score['headline'] = headline\n",
        "        results.append(pol_score)\n",
        "    ##Concatenating this list to our original dataframe, only interested in the values of the ‘compound’ variable.\n",
        "    yellow_preprocess_df['Sentiment_Score'] = pd.DataFrame(results)['compound']                                \n",
        "\n",
        "    print('Yellowness factor preprocessing done!!')\n",
        "    \n",
        "    ##Convert preprocessed df to csv\n",
        "    yellow_preprocess_df.to_csv('csv/yellow_preprocess_df.csv')\n",
        "\n",
        "\n",
        "def invoke_modeling_yellowness():\n",
        "  yellow_preprocess_df = pd.read_csv('csv/yellow_preprocess_df.csv')\n",
        "  X_test = yellow_preprocess_df[['flesch','is_q','is_exclam','is_money','starts_num','All_Caps','num_words','Sentiment_Score']]\n",
        "  yellowness_preds = []\n",
        "  yellowness_model = pickle.load( open( \"models/yellow_model.sav\", \"rb\" ) )\n",
        "  yellowness_preds = yellowness_model.predict(X_test)\n",
        "  yellow_preprocess_df['yellow_factor'] = yellowness_preds\n",
        "  yellow_preprocess_df.to_csv('csv/yellow_df.csv')\n",
        "  \n",
        "  print(\"loading done\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLBrzphqh8rq"
      },
      "source": [
        "## Deception Factor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxWzLZ-3h_Km"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
        "\n",
        "def deception_preprocessing(preprocess_df):\n",
        "  \n",
        "  print(\"Starting preprocessing for deception factor on our text..\")\n",
        "  preprocess_df.Summary = preprocess_df.Summary.apply(str)\n",
        "\n",
        "  ##Creating a feature to count the number of words\n",
        "  preprocess_df['lex_count'] = preprocess_df['Summary'].apply(textstat.lexicon_count)\n",
        "\n",
        "  ##Creating a feature to count the number of difficult words\n",
        "  # preprocess_df['difficult_count'] = preprocess_df['Summary'].apply(textstat.difficult_words)\n",
        "\n",
        "  ##Creating a feature to count the number of sentences\n",
        "  preprocess_df['num_sentences'] = preprocess_df['Summary'].apply(textstat.sentence_count)\n",
        "\n",
        "  ##Creating a feature to count the number of syllable\n",
        "  preprocess_df['num_syllables'] = preprocess_df['Summary'].apply(textstat.syllable_count)\n",
        "\n",
        "  ##Creating a feature for flesch_kincaid_grade\n",
        "  preprocess_df['flesch_kincaid_grade'] = preprocess_df['Summary'].apply(textstat.flesch_kincaid_grade)\n",
        "\n",
        "  ##The Fog Scale (Gunning FOG Formula)\n",
        "  preprocess_df['gunning_fog'] = preprocess_df['Summary'].apply(textstat.gunning_fog)\n",
        "\n",
        "  ##Creating a feature for avg number of syllables per word - \n",
        "  preprocess_df['avg_syllables'] = preprocess_df['num_syllables'] / preprocess_df['lex_count']\n",
        "\n",
        "  ## Average sentence length\n",
        "  preprocess_df['avg_sentence_length'] = preprocess_df['lex_count'] / preprocess_df['num_sentences']\n",
        "\n",
        "  ##Looping to pass every headline into the analyser, a sentiment score is assigned to each headline.\n",
        "  sia = SIA()\n",
        "  results = []\n",
        "  for headline in preprocess_df['Summary']:\n",
        "      pol_score = sia.polarity_scores(headline)\n",
        "      pol_score['headline'] = headline\n",
        "      results.append(pol_score)\n",
        "\n",
        "  ##Concatenating this list to our original dataframe, only interested in the values of the ‘compound’ variable.\n",
        "  preprocess_df['Sentiment_Score'] = pd.DataFrame(results)['compound']\n",
        "\n",
        "  print('Preprocessing done!!')\n",
        "  preprocess_df.to_csv('csv/deception_preprocess_df.csv')\n",
        "\n",
        "\n",
        "def invoke_modeling():\n",
        "  deception_preprocess_df = pd.read_csv('csv/deception_preprocess_df.csv')\n",
        "  X_test = deception_preprocess_df[['avg_sentence_length','avg_syllables','flesch_kincaid_grade','gunning_fog','Sentiment_Score']]\n",
        "  deception_preds = []\n",
        "  deception_model = pickle.load( open( \"models/deception_model.sav\", \"rb\" ) )\n",
        "  deception_preds = deception_model.predict(X_test)\n",
        "  deception_preprocess_df['deceptiveness_factor'] = deception_preds\n",
        "  deception_preprocess_df.to_csv('csv/deception_df.csv')\n",
        "  print(\"loading done\")\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zio6uehUi-Xp"
      },
      "source": [
        "## Style Based Factor "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbAggeS4i8uo"
      },
      "source": [
        "def calculate_style_factor():\n",
        "  ##Read the df\n",
        "  deception_df = pd.read_csv('csv/deception_df.csv')\n",
        "  yellow_df = pd.read_csv('csv/yellow_df.csv')\n",
        "  deception_df = deception_df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'], axis = 1)\n",
        "  yellow_df = yellow_df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'], axis = 1)\n",
        "\n",
        "  ##Creating a df with latent manifold for visualization purposes\n",
        "  style_df = pd.merge(deception_df, yellow_df,  on = ['Posted','Link','Summary','Headline','Source','Sentiment_Score'])\n",
        "\n",
        "  ##Label encoding\n",
        "  style_df['deceptiveness_factor'].replace(\n",
        "      {\n",
        "          'Not Deceptive': 0,\n",
        "          'Deceptive': 1\n",
        "      }, inplace = True\n",
        "  )\n",
        "  style_df['yellow_factor'].replace(\n",
        "      {\n",
        "          'Least yellow': 0,\n",
        "          'Moderately Yellow': 1,\n",
        "          'Highly yellow': 2\n",
        "      }, inplace = True\n",
        "  )\n",
        "\n",
        "  ##Applying a simple ensemble \n",
        "  style_df['badStyle_score'] = style_df['deceptiveness_factor']*0.4 + style_df['yellow_factor']*0.6\n",
        "\n",
        "  streaming_df = pd.read_csv('csv/results_streaming.csv')\n",
        "  streaming_df = pd.merge(streaming_df, style_df,  on = ['Posted','Link','Summary','Headline','Source'])\n",
        "  streaming_df = streaming_df[['Posted','Link','Summary','Headline','Source','AuthenticityScore','credibility_factor','badStyle_score']]\n",
        "  streaming_df.to_csv('csv/results_streaming.csv')\n",
        "  print(streaming_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9acTVqenn3zs"
      },
      "source": [
        "# **Streaming data - Scraping Yahoo news**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE9CRVKTxVaF"
      },
      "source": [
        "import re\n",
        "import csv\n",
        "from time import sleep\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import sys\n",
        "from newspaper import Article\n",
        "import pandas as pd\n",
        "\n",
        "headers = {\n",
        "    'accept': '*/*',\n",
        "    'accept-encoding': 'gzip, deflate, br',\n",
        "    'accept-language': 'en-US,en;q=0.9',\n",
        "    'referer': 'https://www.google.com',\n",
        "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36 Edg/85.0.564.44'\n",
        "}\n",
        "\n",
        "def get_article(card):\n",
        "    \"\"\"Extract article information from the raw html\"\"\"\n",
        "    headline = card.find('h4', 's-title').text\n",
        "    source = card.find(\"span\", 's-source').text\n",
        "    posted = card.find('span', 's-time').text.replace('·', '').strip()\n",
        "    #description = card.find('p', 's-desc').text.strip()\n",
        "    raw_link = card.find('a').get('href')\n",
        "    unquoted_link = requests.utils.unquote(raw_link)\n",
        "    pattern = re.compile(r'RU=(.+)\\/RK')\n",
        "    clean_link = re.search(pattern, unquoted_link).group(1)\n",
        "    #Creating an instance of the article\n",
        "    parsed_article = Article(clean_link, language=\"en\")\n",
        "    #To download the article\n",
        "    try:\n",
        "      parsed_article.download()\n",
        "      #To parse the article\n",
        "      parsed_article.parse()\n",
        "      #To perform natural language processing ie..nlp\n",
        "      parsed_article.nlp()\n",
        "    except:\n",
        "      e = sys.exc_info()[0]\n",
        "    article = (headline, source, posted, clean_link, parsed_article.summary)\n",
        "    return article\n",
        "\n",
        "def get_the_news(search):\n",
        "    \"\"\"Run the main program\"\"\"\n",
        "    template = 'https://news.search.yahoo.com/search?p={}'\n",
        "    url = template.format(search)\n",
        "    articles = []\n",
        "    links = set()\n",
        "    \n",
        "    for i in range(1):\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        cards = soup.find_all('div', 'NewsArticle')\n",
        "        \n",
        "        # extract articles from page\n",
        "        for card in cards:\n",
        "            article = get_article(card)\n",
        "            link = article[-1]\n",
        "            if not link in links:\n",
        "                links.add(link)\n",
        "                articles.append(article)        \n",
        "                \n",
        "        try:\n",
        "            url = soup.find('a', 'next').get('href')\n",
        "            sleep(1)\n",
        "        except AttributeError:\n",
        "            continue\n",
        "            \n",
        "    # save article data\n",
        "    with open('csv/results_streaming.csv', 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Headline', 'Source', 'Posted', 'Link', 'Summary'])\n",
        "        writer.writerows(articles)\n",
        "        \n",
        "    return articles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwe6s85agZSg"
      },
      "source": [
        "# **Team - Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNjY2IW0zL_j"
      },
      "source": [
        "## Team polynomial\n",
        "def generate_class(val):\n",
        "  team_df = pd.read_csv('csv/results_streaming.csv')\n",
        "\n",
        "  if(val >= 0 and val <=0.20):\n",
        "    return 'pants-fire'\n",
        "  elif (val > 0.20 and val <=0.40):\n",
        "    return 'false'\n",
        "  elif (val > 0.40 and val <=0.60):\n",
        "    return 'barely-true'\n",
        "  elif (val > 0.60 and val <=0.70):\n",
        "    return 'half-true'\n",
        "  elif (val > 0.70 and val <=0.80):\n",
        "    return 'mostly-true'\n",
        "  elif (val > 0.80 and val <=1):\n",
        "    return 'true'\n",
        "\n",
        "def truth_o_meter():\n",
        "  team_df = pd.read_csv('csv/results_streaming.csv')\n",
        "  team_df = team_df.drop(columns=['Unnamed: 0'],axis = 1)\n",
        "  team_df['credibility_factor'].replace(\n",
        "        {\n",
        "            'Least credible': 0,\n",
        "            'Moderately credible': 1,\n",
        "            'Highly credible': 2\n",
        "        }, inplace = True\n",
        "    )\n",
        "\n",
        "  team_df['team_score'] = team_df['AuthenticityScore'] + team_df['credibility_factor'] + 1/team_df['badStyle_score']\n",
        "  team_df['team_score'] = (team_df['team_score'] - team_df['team_score'].min()) / (team_df['team_score'].max() - team_df['team_score'].min())    \n",
        "  team_df['truth_o_meter'] = team_df['team_score'].apply(lambda x : generate_class(x))\n",
        "  print(tabulate(team_df, headers='keys', tablefmt='psql'))\n",
        "\n",
        "  team_df.to_csv('csv/results_streaming.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1356b1f891ed46deb4cabd99e85ba0b4",
            "4204160f4bf241878acb8b0c57bb2b5d",
            "a27e24b284c84533afe81fdd2f34ae1a",
            "10dc9fef0b6a464280646650be1a118e",
            "be9d7e1691484a09a561547b5fb8d115"
          ]
        },
        "id": "SxO9i7xwwGGL",
        "outputId": "ba9b0c60-6adb-4357-8ce2-c55f139c6298"
      },
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import functools\n",
        "import textstat\n",
        "import pickle\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "button = widgets.Button(description=\"Get Feed\")\n",
        "output = widgets.Output()\n",
        "\n",
        "def pipeline_team(streaming_df):\n",
        "  model = Pipeline([\n",
        "    ('yellowness_preprocessing', yellowness_preprocessing(streaming_df)),\n",
        "    ('deception_preprocessing', deception_preprocessing(streaming_df)),\n",
        "    ('authenticity_preprocessing', authenticity_preprocessing(streaming_df)),\n",
        "    ('credibility_preprocessing', credibility_preprocessing(streaming_df)),\n",
        "    ('mlp', invoke_modeling_yellowness()),\n",
        "    ('mlp_1', invoke_modeling()),\n",
        "    ('xgboost_1', invoke_modeling_authenticity()),\n",
        "    ('xgboost', invoke_modeling_credibility()),\n",
        "    ('calculate_style_score', calculate_style_factor()),\n",
        "    ('truth_o_meter',truth_o_meter())\n",
        "  ])\n",
        "\n",
        "##On clicking the button\n",
        "def on_button_clicked(b):\n",
        "\n",
        "  print(\"Fetching data.. please wait!\")\n",
        "  news_list = [\"latest\"]\n",
        "  for news in news_list:\n",
        "      ##Invoke function 'get_the_news'\n",
        "      articles_final = get_the_news(news)\n",
        "  print('fetching data done!!')\n",
        "  ##Reading the above created csv with latest news\n",
        "  streaming_df = pd.read_csv('csv/results_streaming.csv')\n",
        "  pipeline_team(streaming_df)\n",
        "  \n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "display(button, output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1356b1f891ed46deb4cabd99e85ba0b4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Button(description='Get Feed', style=ButtonStyle())"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10dc9fef0b6a464280646650be1a118e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Fetching data.. please wait!\n",
            "fetching data done!!\n",
            "Starting preprocessing for yellowness factor on our text..\n",
            "Yellowness factor preprocessing done!!\n",
            "Starting preprocessing for deception factor on our text..\n",
            "Preprocessing done!!\n",
            "authenticity preprocessing done!!\n",
            "loading done\n",
            "loading done\n",
            "autheticity score calculation done!\n",
            "           Posted  ... AuthenticityScore\n",
            "0  21 minutes ago  ...                 2\n",
            "1      1 hour ago  ...                 0\n",
            "2     3 hours ago  ...                 0\n",
            "3    19 hours ago  ...                 0\n",
            "4     2 hours ago  ...                 0\n",
            "5    14 hours ago  ...                 0\n",
            "6     3 hours ago  ...                 0\n",
            "\n",
            "[7 rows x 6 columns]\n",
            "           Posted  ...   credibility_factor\n",
            "0  21 minutes ago  ...  Moderately credible\n",
            "1      1 hour ago  ...  Moderately credible\n",
            "2     3 hours ago  ...  Moderately credible\n",
            "3    19 hours ago  ...  Moderately credible\n",
            "4     2 hours ago  ...      Highly credible\n",
            "5    14 hours ago  ...       Least credible\n",
            "6     3 hours ago  ...  Moderately credible\n",
            "\n",
            "[7 rows x 7 columns]\n",
            "           Posted  ... badStyle_score\n",
            "0  21 minutes ago  ...            0.4\n",
            "1      1 hour ago  ...            0.6\n",
            "2     3 hours ago  ...            1.0\n",
            "3    19 hours ago  ...            0.4\n",
            "4     2 hours ago  ...            0.4\n",
            "5    14 hours ago  ...            0.4\n",
            "6     3 hours ago  ...            0.4\n",
            "\n",
            "[7 rows x 8 columns]\n",
            "+----+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+------------------------------+---------------------+----------------------+------------------+--------------+-----------------+\n",
            "|    | Posted         | Link                                                                                                                                              | Summary                                                                                                                                                                                                   | Headline                                                                                       | Source                       |   AuthenticityScore |   credibility_factor |   badStyle_score |   team_score | truth_o_meter   |\n",
            "|----+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+------------------------------+---------------------+----------------------+------------------+--------------+-----------------|\n",
            "|  0 | 21 minutes ago | https://www.msn.com/en-ca/lifestyle/smart-living/the-latest-gathering-of-muslim-nations-condemns-israel/ar-BB1gBKe3                               | The latest round of fighting comes as Turkey has reportedly been seeking to restore ties with Israel.                                                                                                     | The Latest: Gathering of Muslim nations condemns Israel                                        | MSN News                     |                   2 |                    1 |              0.4 |     1        | true            |\n",
            "|    |                |                                                                                                                                                   | Since Monday, Gaza militants have fired hundreds of rockets toward Israel.                                                                                                                                |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|    |                |                                                                                                                                                   | ___CAIRO — An Egyptian intelligence official says Egypt is engaged in “intensive” talks with Israel and Gaza militants on reaching a cease-fire to end the latest round of fighting.                      |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|    |                |                                                                                                                                                   | —Samy Magdy in Cairo___GAZA CITY, Gaza Strip — An Israeli airstrike has hit another high-rise building in the middle of Gaza City.                                                                        |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|    |                |                                                                                                                                                   | Since Monday, Gaza militants have also fired hundreds of rockets toward Israel.                                                                                                                           |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|  1 | 1 hour ago     | https://investorplace.com/2021/05/latest-dogecoin-news-elon-musks-tesla-tweet-excites-the-doge-army-on-tuesday/                                   | nan                                                                                                                                                                                                       | Latest Dogecoin News: Elon Musk's Tesla Tweet Excites the Doge Army on Tuesday                 | InvestorPlace                |                   0 |                    1 |              0.6 |     0.190476 | pants-fire      |\n",
            "|  2 | 3 hours ago    | https://abcnews.go.com/US/latest-phone-scam-fake-dea-agent-steal-money/story?id=77495121                                                          | Authorities are working to crack down on an impostor phone scam in which the caller pretends to be a DEA agent and then tries to steal the victim's money -- often by having them purchase gift cards.    | In latest phone scam, a fake DEA agent tries to steal your money using gift cards              | ABC News                     |                   0 |                    1 |              1   |     0        | pants-fire      |\n",
            "|    |                |                                                                                                                                                   | \"The fake DEA agent will convince people to turn over money in a couple different ways.                                                                                                                   |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|    |                |                                                                                                                                                   | The scammers told her they were going to freeze her bank accounts unless she withdrew her money and transferred it to gift cards.                                                                         |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|    |                |                                                                                                                                                   | Pierogi said that scammers use gift cards as a preferred method of payment because they have a very quick turnaround.                                                                                     |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|    |                |                                                                                                                                                   | The FTC said that the most popular gift cards requested by scammers include eBay, Google Play and iTunes cards.                                                                                           |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|  3 | 19 hours ago   | https://news.yahoo.com/pursuit-love-latest-bbc-show-192806002.html                                                                                | Television The Pursuit of Love - Robert Viglasky /BBCThe Pursuit of Love has become the latest BBC drama to be marred by sound problems, with viewers unable to make out what the characters were saying. | Pursuit of Love latest BBC show to face complaints over dialled-down dialogue                  | The Telegraph via Yahoo News |                   0 |                    1 |              0.4 |     0.428571 | barely-true     |\n",
            "|    |                |                                                                                                                                                   | Rousham House and Gardens in Oxfordshire - Nigel Francis/AlamyCharles and Angela Cottrell-Dormer tuned in to the episode on Sunday night, eager to see their house on screen.                             |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|    |                |                                                                                                                                                   | One viewer wrote: “I find the extremes of sound on The Pursuit of Love excruciating.                                                                                                                      |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|    |                |                                                                                                                                                   | The episode sparked online interest in the locations, including Rousham House, north of Oxford, which stood in for Alconleigh.                                                                            |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|    |                |                                                                                                                                                   | The Pursuit of Love, review: a jolly fun adaptation – just a shame no one told its leading lady                                                                                                           |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|  4 | 2 hours ago    | https://www.thetimesherald.com/story/news/2021/05/11/city-council-rejects-latest-marijuana-petition-but-its-still-headed-aug-3-ballot/5032048001/ | Another marijuana ballot proposal is set to go before Port Huron voters on Aug. 3 — this time asking them to change a measure the majority already approved last November.                                | Port Huron officials reject latest marijuana petition, but it's still headed for Aug. 3 ballot | Times Herald                 |                   0 |                    2 |              0.4 |     0.714286 | mostly-true     |\n",
            "|    |                |                                                                                                                                                   | The group’s representatives have alleged the latest proposal to change its original ordinance is to put a bigger emphasis on medical marijuana licensure and prevent future lawsuits against the city.    |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|    |                |                                                                                                                                                   | Under the city’s charter, if council decides against adopting a petition ordinance, as it did Monday, it automatically heads to an election ballot.                                                       |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|    |                |                                                                                                                                                   | That means Progress for Michigan meets the deadline Tuesday to get a measure on the August ballot.                                                                                                        |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|    |                |                                                                                                                                                   | Although the city had already adopted an ordinance to award recreational marijuana permits last fall, Progress for Michigan compelled its first proposal on the November ballot through court action.     |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|  5 | 14 hours ago   | https://www.msn.com/en-ca/news/canada/the-latest-numbers-on-covid-19-in-canada-for-monday-may-10-2021/ar-BB1gAusV                                 | © Provided by The Canadian PressThe latest numbers of confirmed COVID-19 cases in Canada as of 9:00 p.m.                                                                                                  | The latest numbers on COVID-19 in Canada for Monday, May 10, 2021                              | MSN News                     |                   0 |                    0 |              0.4 |     0.142857 | pants-fire      |\n",
            "|    |                |                                                                                                                                                   | The seven-day rolling average of new reported deaths is 49.                                                                                                                                               |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|    |                |                                                                                                                                                   | The seven-day rolling average of new cases is five.                                                                                                                                                       |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|    |                |                                                                                                                                                   | The seven-day rolling average of new cases is one.                                                                                                                                                        |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|    |                |                                                                                                                                                   | The seven-day rolling average of new reported deaths is one.                                                                                                                                              |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|  6 | 3 hours ago    | https://www.rawstory.com/lumber-shortage-2021/                                                                                                    | Amateur sleuths are racking up page views by staking out hardware stores and filming stacks of lumber in the latest online conspiracy theory.                                                             | Right-wingers staking out hardware stores in pursuit of latest anti-Biden conspiracy theory    | The Raw Story                |                   0 |                    1 |              0.4 |     0.428571 | barely-true     |\n",
            "|    |                |                                                                                                                                                   | \"We're still seeing the prices increase at the lumber yards, so I'm not sure why.\"                                                                                                                        |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|    |                |                                                                                                                                                   | Why,\" has been viewed nearly 500,000 times and was cited by the pro-Donald Trump blog Zero Hedge as proof that lumber prices were being artificially inflated.                                            |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|    |                |                                                                                                                                                   | \"There are a LOT of these type vids showing the BS narrative of Lumber shortages,\" wrote one person on a popular QAnon forum.                                                                             |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "|    |                |                                                                                                                                                   | \"TikTok, these folks are lying to us about this wood shortage,\" wrote a TikTok user with the screen name \"Red the Trucker.\"                                                                               |                                                                                                |                              |                     |                      |                  |              |                 |\n",
            "+----+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+------------------------------+---------------------+----------------------+------------------+--------------+-----------------+\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}