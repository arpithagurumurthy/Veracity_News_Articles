{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgFlACgNXvAI"
      },
      "source": [
        "YAHOO News is a news aggregation website thats collects news from various sources\n",
        "\n",
        "This notebook implements scraping Yahoo news using beautiful soup and newspaper module\n",
        "Newspaper is a Python module used for extracting and parsing newspaper articles.\n",
        "\n",
        "**References**:\n",
        "* https://www.geeksforgeeks.org/newspaper-article-scraping-curation-python/\n",
        "* https://github.com/israel-dryer/Yahoo-News-Scraper/blob/master/yahoo-news-scraper.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWSUVXfHbvKY",
        "outputId": "5d7ca52f-8f38-4848-c411-b23f6632a664"
      },
      "source": [
        "pip install newspaper3k"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 11.3MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 16.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 30kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 40kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 51kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 61kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 71kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 81kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 102kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 112kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 122kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 133kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 143kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 153kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 163kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 174kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 184kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 194kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 204kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 215kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\n",
            "Collecting jieba3k>=0.35.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 6.9MB/s \n",
            "\u001b[?25hCollecting feedparser>=5.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/21/faf1bac028662cc8adb2b5ef7a6f3999a765baa2835331df365289b0ca56/feedparser-6.0.2-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.1)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\n",
            "Collecting tldextract>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/62/b6acd3129c5615b9860e670df07fd55b76175b63e6b7f68282c7cad38e9e/tldextract-3.1.0-py2.py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n",
            "Collecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.5.3->newspaper3k) (1.15.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.0.12)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Building wheels for collected packages: jieba3k, tinysegmenter, feedfinder2, sgmllib3k\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp37-none-any.whl size=7398406 sha256=1511724f2f0e2b29d2727cf306513162c64bf96864ceef83de1a9eca6887c8ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp37-none-any.whl size=13538 sha256=247e0f5884ccbb432d960c0ddcc4b55b8a2d11be531acad7be3c051d70255618\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp37-none-any.whl size=3358 sha256=cb800fd87003cf2acaf69c276472f9e30081cee79cbe1bdb8c1fdd9446a7d6f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp37-none-any.whl size=6067 sha256=bc46f442f13ccb0f1a56ab795c2cef7cf24d588ed987e847fcaabd5592cb8610\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built jieba3k tinysegmenter feedfinder2 sgmllib3k\n",
            "Installing collected packages: jieba3k, sgmllib3k, feedparser, tinysegmenter, requests-file, tldextract, cssselect, feedfinder2, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.2 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzGCPhOZcJ31",
        "outputId": "c380397e-c2cb-4e17-9ceb-04f55addf542"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vnqm_fxvciRX"
      },
      "source": [
        "import re\n",
        "import csv\n",
        "from time import sleep\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import sys\n",
        "from newspaper import Article\n",
        "\n",
        "headers = {\n",
        "    'accept': '*/*',\n",
        "    'accept-encoding': 'gzip, deflate, br',\n",
        "    'accept-language': 'en-US,en;q=0.9',\n",
        "    'referer': 'https://www.google.com',\n",
        "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36 Edg/85.0.564.44'\n",
        "}\n",
        "\n",
        "def get_article(card):\n",
        "    \"\"\"Extract article information from the raw html\"\"\"\n",
        "    headline = card.find('h4', 's-title').text\n",
        "    source = card.find(\"span\", 's-source').text\n",
        "    posted = card.find('span', 's-time').text.replace('·', '').strip()\n",
        "    #description = card.find('p', 's-desc').text.strip()\n",
        "    raw_link = card.find('a').get('href')\n",
        "    unquoted_link = requests.utils.unquote(raw_link)\n",
        "    pattern = re.compile(r'RU=(.+)\\/RK')\n",
        "    clean_link = re.search(pattern, unquoted_link).group(1)\n",
        "    #Creating an instance of the article\n",
        "    parsed_article = Article(clean_link, language=\"en\")\n",
        "    #To download the article\n",
        "    try:\n",
        "      parsed_article.download()\n",
        "      #To parse the article\n",
        "      parsed_article.parse()\n",
        "      #To perform natural language processing ie..nlp\n",
        "      parsed_article.nlp()\n",
        "    except:\n",
        "      e = sys.exc_info()[0]\n",
        "    article = (headline, source, posted, clean_link, parsed_article.summary)\n",
        "    return article\n",
        "\n",
        "def get_the_news(search):\n",
        "    \"\"\"Run the main program\"\"\"\n",
        "    template = 'https://news.search.yahoo.com/search?p={}'\n",
        "    url = template.format(search)\n",
        "    articles = []\n",
        "    links = set()\n",
        "    \n",
        "    for i in range(200):\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        cards = soup.find_all('div', 'NewsArticle')\n",
        "        \n",
        "        # extract articles from page\n",
        "        for card in cards:\n",
        "            article = get_article(card)\n",
        "            link = article[-1]\n",
        "            if not link in links:\n",
        "                links.add(link)\n",
        "                articles.append(article)        \n",
        "                \n",
        "        try:\n",
        "            url = soup.find('a', 'next').get('href')\n",
        "            sleep(1)\n",
        "        except AttributeError:\n",
        "            continue\n",
        "            \n",
        "    # save article data\n",
        "    with open('results.csv', 'a', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Headline', 'Source', 'Posted', 'Link', 'Summary'])\n",
        "        writer.writerows(articles)\n",
        "        \n",
        "    return articles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7brb8205drhQ",
        "outputId": "a0dac74f-f107-44ed-8dfa-73a36a515b47"
      },
      "source": [
        "# run the main program\n",
        "articles_final = []\n",
        "news_list = [\"corona\",\"politics\",\"stocks\"]\n",
        "for news in news_list:\n",
        "  articles_final = get_the_news(news)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/dateutil/parser/_parser.py:1218: UnknownTimezoneWarning: tzname CDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
            "  category=UnknownTimezoneWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/dateutil/parser/_parser.py:1218: UnknownTimezoneWarning: tzname PDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
            "  category=UnknownTimezoneWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/dateutil/parser/_parser.py:1218: UnknownTimezoneWarning: tzname EDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
            "  category=UnknownTimezoneWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/dateutil/parser/_parser.py:1218: UnknownTimezoneWarning: tzname MST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
            "  category=UnknownTimezoneWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/dateutil/parser/_parser.py:1218: UnknownTimezoneWarning: tzname MDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
            "  category=UnknownTimezoneWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khC_g56RY8WJ"
      },
      "source": []
    }
  ]
}